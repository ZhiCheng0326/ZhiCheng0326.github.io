---
title: 'Making Large Language Models Perform Better in
Knowledge Graph Completion'
date: 2023-10-31
permalink: /posts/2023/10/2023-10-31_llm_kg/
tags:
  - LLM
  - Knowledge graph
---

This paper proposes **knowledge prefix adapter (KoPA)**, an approach to improve the **reasoning** ability of LLM by incorporating the **structural information** of knowledge graph (KG). 

<div role="note" style="display: flex;"><div style="display: flex; width: 100%; border-radius: 4px; background: rgb(241, 241, 239); padding: 16px 16px 16px 12px;"><div><div role="button" tabindex="0" class="notion-record-icon notranslate" aria-label="üí° Change callout icon" style="user-select: none; transition: background 20ms ease-in 0s; cursor: pointer; display: flex; align-items: center; justify-content: center; height: 24px; width: 24px; border-radius: 0.25em; flex-shrink: 0;"><div style="display: flex; align-items: center; justify-content: center; height: 24px; width: 24px;"><div style="height: 21.6px; width: 21.6px; font-size: 21.6px; line-height: 1; margin-left: 0px; color: black;"><span role="img" aria-label="üí°" style="font-family: &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, NotoColorEmoji, &quot;Noto Color Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Android Emoji&quot;, EmojiSymbols; line-height: 1em; white-space: nowrap;">üí°</span></div></div></div></div><div style="display: flex; flex-direction: column; min-width: 0px; margin-left: 8px; width: 100%;"><details><summary>Table of Contents</summary><div data-block-id="05697a4b-49b6-4915-92c7-f03457265954" class="notion-selectable notion-table_of_contents-block" style="width: 100%; max-width: 100%; margin-top: 4px; margin-bottom: 0px;"><div contenteditable="false" data-content-editable-void="true" style="position: relative;"><div style="color: rgb(120, 119, 116); fill: rgb(120, 119, 116);"><a href="#abstract" rel="noopener noreferrer" style="display: block; color: inherit; text-decoration: none; user-select: none; transition: background 20ms ease-in 0s; cursor: pointer;"><div style="padding: 6px 2px; font-size: 14px; line-height: 1.3; display: flex; align-items: center; margin-left: 0px;"><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; background-image: linear-gradient(to right, rgba(55, 53, 47, 0.16) 0%, rgba(55, 53, 47, 0.16) 100%); background-repeat: repeat-x; background-position: 0px 100%; background-size: 100% 1px;">Abstract</div></div></a></div><div style="color: rgb(120, 119, 116); fill: rgb(120, 119, 116);"><a href="#problem-statement" rel="noopener noreferrer" style="display: block; color: inherit; text-decoration: none; user-select: none; transition: background 20ms ease-in 0s; cursor: pointer;"><div style="padding: 6px 2px; font-size: 14px; line-height: 1.3; display: flex; align-items: center; margin-left: 0px;"><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; background-image: linear-gradient(to right, rgba(55, 53, 47, 0.16) 0%, rgba(55, 53, 47, 0.16) 100%); background-repeat: repeat-x; background-position: 0px 100%; background-size: 100% 1px;">Problem Statement</div></div></a></div><div style="color: rgb(120, 119, 116); fill: rgb(120, 119, 116);"><a href="#introduction" rel="noopener noreferrer" style="display: block; color: inherit; text-decoration: none; user-select: none; transition: background 20ms ease-in 0s; cursor: pointer;"><div style="padding: 6px 2px; font-size: 14px; line-height: 1.3; display: flex; align-items: center; margin-left: 0px;"><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; background-image: linear-gradient(to right, rgba(55, 53, 47, 0.16) 0%, rgba(55, 53, 47, 0.16) 100%); background-repeat: repeat-x; background-position: 0px 100%; background-size: 100% 1px;">Introduction</div></div></a></div><div style="color: rgb(120, 119, 116); fill: rgb(120, 119, 116);"><a href="#notation--preliminaries" rel="noopener noreferrer" style="display: block; color: inherit; text-decoration: none; user-select: none; transition: background 20ms ease-in 0s; cursor: pointer;"><div style="padding: 6px 2px; font-size: 14px; line-height: 1.3; display: flex; align-items: center; margin-left: 24px;"><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; background-image: linear-gradient(to right, rgba(55, 53, 47, 0.16) 0%, rgba(55, 53, 47, 0.16) 100%); background-repeat: repeat-x; background-position: 0px 100%; background-size: 100% 1px;">Notation &amp; Preliminaries</div></div></a></div><div style="color: rgb(120, 119, 116); fill: rgb(120, 119, 116);"><a href="#training-free-reasoning-approaches" rel="noopener noreferrer" style="display: block; color: inherit; text-decoration: none; user-select: none; transition: background 20ms ease-in 0s; cursor: pointer;"><div style="padding: 6px 2px; font-size: 14px; line-height: 1.3; display: flex; align-items: center; margin-left: 24px;"><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; background-image: linear-gradient(to right, rgba(55, 53, 47, 0.16) 0%, rgba(55, 53, 47, 0.16) 100%); background-repeat: repeat-x; background-position: 0px 100%; background-size: 100% 1px;">Training-free Reasoning Approaches</div></div></a></div><div style="color: rgb(120, 119, 116); fill: rgb(120, 119, 116);"><a href="#zero-shot-reasoning-approach" rel="noopener noreferrer" style="display: block; color: inherit; text-decoration: none; user-select: none; transition: background 20ms ease-in 0s; cursor: pointer;"><div style="padding: 6px 2px; font-size: 14px; line-height: 1.3; display: flex; align-items: center; margin-left: 48px;"><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; background-image: linear-gradient(to right, rgba(55, 53, 47, 0.16) 0%, rgba(55, 53, 47, 0.16) 100%); background-repeat: repeat-x; background-position: 0px 100%; background-size: 100% 1px;">Zero-shot reasoning approach</div></div></a></div><div style="color: rgb(120, 119, 116); fill: rgb(120, 119, 116);"><a href="#in-context-learning-approach" rel="noopener noreferrer" style="display: block; color: inherit; text-decoration: none; user-select: none; transition: background 20ms ease-in 0s; cursor: pointer;"><div style="padding: 6px 2px; font-size: 14px; line-height: 1.3; display: flex; align-items: center; margin-left: 48px;"><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; background-image: linear-gradient(to right, rgba(55, 53, 47, 0.16) 0%, rgba(55, 53, 47, 0.16) 100%); background-repeat: repeat-x; background-position: 0px 100%; background-size: 100% 1px;">In-context learning approach</div></div></a></div><div style="color: rgb(120, 119, 116); fill: rgb(120, 119, 116);"><a href="#instruction-tuning-approach" rel="noopener noreferrer" style="display: block; color: inherit; text-decoration: none; user-select: none; transition: background 20ms ease-in 0s; cursor: pointer;"><div style="padding: 6px 2px; font-size: 14px; line-height: 1.3; display: flex; align-items: center; margin-left: 24px;"><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; background-image: linear-gradient(to right, rgba(55, 53, 47, 0.16) 0%, rgba(55, 53, 47, 0.16) 100%); background-repeat: repeat-x; background-position: 0px 100%; background-size: 100% 1px;">Instruction-tuning Approach</div></div></a></div><div style="color: rgb(120, 119, 116); fill: rgb(120, 119, 116);"><a href="#vanilla-instruction-tuning" rel="noopener noreferrer" style="display: block; color: inherit; text-decoration: none; user-select: none; transition: background 20ms ease-in 0s; cursor: pointer;"><div style="padding: 6px 2px; font-size: 14px; line-height: 1.3; display: flex; align-items: center; margin-left: 48px;"><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; background-image: linear-gradient(to right, rgba(55, 53, 47, 0.16) 0%, rgba(55, 53, 47, 0.16) 100%); background-repeat: repeat-x; background-position: 0px 100%; background-size: 100% 1px;">Vanilla instruction tuning</div></div></a></div><div style="color: rgb(120, 119, 116); fill: rgb(120, 119, 116);"><a href="#structural-aware-instruction-tuning" rel="noopener noreferrer" style="display: block; color: inherit; text-decoration: none; user-select: none; transition: background 20ms ease-in 0s; cursor: pointer;"><div style="padding: 6px 2px; font-size: 14px; line-height: 1.3; display: flex; align-items: center; margin-left: 48px;"><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; background-image: linear-gradient(to right, rgba(55, 53, 47, 0.16) 0%, rgba(55, 53, 47, 0.16) 100%); background-repeat: repeat-x; background-position: 0px 100%; background-size: 100% 1px;">Structural-aware instruction tuning</div></div></a></div><div style="color: rgb(120, 119, 116); fill: rgb(120, 119, 116);"><a href="#methodology" rel="noopener noreferrer" style="display: block; color: inherit; text-decoration: none; user-select: none; transition: background 20ms ease-in 0s; cursor: pointer;"><div style="padding: 6px 2px; font-size: 14px; line-height: 1.3; display: flex; align-items: center; margin-left: 0px;"><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; background-image: linear-gradient(to right, rgba(55, 53, 47, 0.16) 0%, rgba(55, 53, 47, 0.16) 100%); background-repeat: repeat-x; background-position: 0px 100%; background-size: 100% 1px;">Methodology</div></div></a></div><div style="color: rgb(120, 119, 116); fill: rgb(120, 119, 116);"><a href="#knowledge-prefix-adapter-kopa" rel="noopener noreferrer" style="display: block; color: inherit; text-decoration: none; user-select: none; transition: background 20ms ease-in 0s; cursor: pointer;"><div style="padding: 6px 2px; font-size: 14px; line-height: 1.3; display: flex; align-items: center; margin-left: 24px;"><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; background-image: linear-gradient(to right, rgba(55, 53, 47, 0.16) 0%, rgba(55, 53, 47, 0.16) 100%); background-repeat: repeat-x; background-position: 0px 100%; background-size: 100% 1px;">Knowledge Prefix Adapter (KoPA)</div></div></a></div><div style="color: rgb(120, 119, 116); fill: rgb(120, 119, 116);"><a href="#step-1-structural-embedding-pre-training" rel="noopener noreferrer" style="display: block; color: inherit; text-decoration: none; user-select: none; transition: background 20ms ease-in 0s; cursor: pointer;"><div style="padding: 6px 2px; font-size: 14px; line-height: 1.3; display: flex; align-items: center; margin-left: 48px;"><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; background-image: linear-gradient(to right, rgba(55, 53, 47, 0.16) 0%, rgba(55, 53, 47, 0.16) 100%); background-repeat: repeat-x; background-position: 0px 100%; background-size: 100% 1px;">Step 1: Structural embedding pre-training</div></div></a></div><div style="color: rgb(120, 119, 116); fill: rgb(120, 119, 116);"><a href="#step-2-knowledge-prefix-adapter" rel="noopener noreferrer" style="display: block; color: inherit; text-decoration: none; user-select: none; transition: background 20ms ease-in 0s; cursor: pointer;"><div style="padding: 6px 2px; font-size: 14px; line-height: 1.3; display: flex; align-items: center; margin-left: 48px;"><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; background-image: linear-gradient(to right, rgba(55, 53, 47, 0.16) 0%, rgba(55, 53, 47, 0.16) 100%); background-repeat: repeat-x; background-position: 0px 100%; background-size: 100% 1px;">Step 2: Knowledge prefix adapter</div></div></a></div><div style="color: rgb(120, 119, 116); fill: rgb(120, 119, 116);"><a href="#step-3-fine-tuning-with-s_kpa" rel="noopener noreferrer" style="display: block; color: inherit; text-decoration: none; user-select: none; transition: background 20ms ease-in 0s; cursor: pointer;"><div style="padding: 6px 2px; font-size: 14px; line-height: 1.3; display: flex; align-items: center; margin-left: 48px;"><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; background-image: linear-gradient(to right, rgba(55, 53, 47, 0.16) 0%, rgba(55, 53, 47, 0.16) 100%); background-repeat: repeat-x; background-position: 0px 100%; background-size: 100% 1px;">Step 3: Fine tuning with $S_{kpa}$</div></div></a></div><div style="color: rgb(120, 119, 116); fill: rgb(120, 119, 116);"><a href="#complexity-analysis" rel="noopener noreferrer" style="display: block; color: inherit; text-decoration: none; user-select: none; transition: background 20ms ease-in 0s; cursor: pointer;"><div style="padding: 6px 2px; font-size: 14px; line-height: 1.3; display: flex; align-items: center; margin-left: 24px;"><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; background-image: linear-gradient(to right, rgba(55, 53, 47, 0.16) 0%, rgba(55, 53, 47, 0.16) 100%); background-repeat: repeat-x; background-position: 0px 100%; background-size: 100% 1px;">Complexity Analysis</div></div></a></div><div style="color: rgb(120, 119, 116); fill: rgb(120, 119, 116);"><a href="#results" rel="noopener noreferrer" style="display: block; color: inherit; text-decoration: none; user-select: none; transition: background 20ms ease-in 0s; cursor: pointer;"><div style="padding: 6px 2px; font-size: 14px; line-height: 1.3; display: flex; align-items: center; margin-left: 0px;"><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; background-image: linear-gradient(to right, rgba(55, 53, 47, 0.16) 0%, rgba(55, 53, 47, 0.16) 100%); background-repeat: repeat-x; background-position: 0px 100%; background-size: 100% 1px;">Results</div></div></a></div><div style="color: rgb(120, 119, 116); fill: rgb(120, 119, 116);"><a href="#ablation-study" rel="noopener noreferrer" style="display: block; color: inherit; text-decoration: none; user-select: none; transition: background 20ms ease-in 0s; cursor: pointer;"><div style="padding: 6px 2px; font-size: 14px; line-height: 1.3; display: flex; align-items: center; margin-left: 24px;"><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; background-image: linear-gradient(to right, rgba(55, 53, 47, 0.16) 0%, rgba(55, 53, 47, 0.16) 100%); background-repeat: repeat-x; background-position: 0px 100%; background-size: 100% 1px;">Ablation study</div></div></a></div><div style="color: rgb(120, 119, 116); fill: rgb(120, 119, 116);"><a href="#reference" rel="noopener noreferrer" style="display: block; color: inherit; text-decoration: none; user-select: none; transition: background 20ms ease-in 0s; cursor: pointer;"><div style="padding: 6px 2px; font-size: 14px; line-height: 1.3; display: flex; align-items: center; margin-left: 0px;"><div class="notranslate" style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis; background-image: linear-gradient(to right, rgba(55, 53, 47, 0.16) 0%, rgba(55, 53, 47, 0.16) 100%); background-repeat: repeat-x; background-position: 0px 100%; background-size: 100% 1px;">Reference</div></div></a></div></div></div></details></div></div></div>

<br />

# Abstract

- The paper discusses how to make LLMs perform better in knowledge graph completion (KGC), which is a task to predict missing triples in knowledge graphs (KGs)
- The paper proposes a **knowledge prefix adapter (KoPA)**, which uses structural embeddings of entities and relations in KGs to inform the LLMs of the KG structure and enable **structural-aware reasoning**.

# Problem Statement

- Transforming KGC tasks into text-based prediction leads to some issues:
    - inadequate memory for precise and nuanced factual knowledge (**hallucination**)
- Current KGC approach apply vanilla instruction tuning, which **neglects the useful structural information**

# Introduction

- KGs form a semantic web containing extensive entities and relations, which model and store real-world knowledge in the **triple form: (head entity, relation, tail entity)**
- Manually edited or automatically extracted knowledge graphs are often incomplete, consisting only of observed knowledge and leaving many missing triples undiscovered. This motivates **knowledge graph completion (KGC)** to **predict missing triples** in a KG.
- Existing KGC approaches are divided into two categories: **embedding-based** methods and **pre-train language** **models** methods (PLM)

## Notation & Preliminaries

- Denote KG, $G = (E, R, T, D)$
,
    - $E$ denotes entity set
    - $R$ denotes relation set
    - $T$ denotes the triple set of head entity, relation, tail entity
    
    $$
    T = \{(h, r, t) | h, t \in E, r \in R\}
    $$
    
    - $D$ denotes text description set of each entity and relation
- Denote LLM as $M$, the input textual sequence $S$ consists of
    - the instruction prompt $I$
    - the triple prompt $X$:
        
        $$
        X(h, r, t) = D(h) \oplus D(r) \oplus D(t)
        
        $$
        
        where $\oplus$ denotes the concatenation operation
        
    - the optional auxiliary demonstration prompt $U$
- To investigate how LLM accomplish KGC task:
    - In LLM paradigm, the model $M$ is expected to  answer ‚Äòtrue‚Äô or ‚Äòfalse‚Äô given the textual sequence input $S = I ‚äï U ‚äï X$

## Training-free Reasoning Approaches

### Zero-shot reasoning approach

- The input sequence denoted as $S_{zsr} = I_{zsr} \oplus X$, without auxiliary information $U$
- The decoding process of LLM $M$ can be formulated as:

$$
A_{zsr} = \arg\max_{A} P_M (A|S_{zsr}) = \arg\max_{A} P_M (A|I_{zsr}, X)

$$

where $A_{zsr}$ is the generated answer

- In this setting, no KG information is added to $S$

### In-context learning approach

- This approach is able to include auxiliary information $U$
- The decoding process of LLM $M$ can be formulated as:

$$
A_{icl} = \arg\max_{A} P_M (A|S_{icl}) = \arg\max_{A} P_M (A|I_{icl}, U, X)

$$

where $A_{icl}$ is the generated answer

- $U$ is the demonstration triple $(h,r,t)$. The author proposed to sample triples that are in the local structure of $h$ and $t$. Both positive triples and negative triples are sampled together.
- Hence, we are able to incorporate the local structure information into $U$  with both positive and negative samples.

## Instruction-tuning Approach

### Vanilla instruction tuning

- $I_{it}$ describes the details of completing the triple classification task
- $X$ describes prompt of the triple
- The input to the model $M$:

$$
S_{ùëñùë°} = I_{ùëñùë°} ‚äïX ‚äïA_{it}
$$

where $A_{it}$ is the predicted answer of the training data

- The model $M$ is fine-tuned with the next word prediction task with the objective:

$$
L_{it} = -\frac{1}{|S_{it}|} \sum_{i=1}^{|S_{it}|} \log P_M (s_i|s_{<i})
$$

where $ùë†_ùëñ$ $\(ùëñ = 1, 2, . . . , \|S_{ùëñùë°}\|\)$ represents the textual tokens of the input sequence $S_{it}$

- Vanilla IT fine-tunes the $M$ to **learn from a single triple**, limiting its ability to utilize KG‚Äôs rich semantics

### Structural-aware instruction tuning

- To incorporate the structural information, the neighborhood of head $‚Ñé$  and tail $t$ are sampled. Next, we put the **textual descriptions of neighborhood triples** in the demonstration prompt $U_{it}$
- Hence, the input training sequence is now $S_{ùëñùë°} = I_{ùëñùë°} ‚äïU_{ùëñùë°} ‚äïX‚äïA_{it}$

# Methodology

Current approaches such as *training-free reasoning approach* and *instruction-tuning approach* has several drawbacks:

- invalid or redundant information
- not scalable and effective because of the lengthy prompt

Hence, this paper propose **knowledge prefix adapter (KoPA)** to leverages self-supervised structural embedding pre-training to capture the structural information in the KG. 

## Knowledge Prefix Adapter (KoPA)

![Untitled](/images/Making%20Large%20Language%20Models%20Perform%20Better%20in%20Kno%205fdf3bcadeca4e1a950a604e114d3ee9/Untitled.png)

- Step 1: we **extract the structural information** of entities and relations from the KG through structural embedding pre-training
- Step 2: Then, we **inject** this structural information **through a structural prefix adapter** into the input sequence $S$.
- Step 3: LLM $M$ is the further tuned with structural-injected sequence.

### Step 1: Structural embedding pre-training

- For each entity $ùëí ‚àà E$ and each relation $ùëü ‚àà R$, we learn a **structural embedding** $ùíÜ ‚àà R^{d_e}$, $ùíì ‚àà R^{d_r}$ respectively, where $ùëë_ùëí, ùëë_ùëü$ are the embedding dimensions.
- We further adapt them into the textual representation space of LLMs
- We define a score function $F (‚Ñé, ùëü, ùë°)$ to measure the plausibility of the triple  $(‚Ñé, ùëü, ùë°)$
- We adopt the self-supervised pre-training objective by negative sampling:

$$
L_{pre} = \frac{1}{|T|} \sum_{(h,r,t) \in T} \left( - \log \sigma(\gamma - F(h, r, t)) - \sum_{i=1}^{K} p_i \log \sigma(F(h'_i, r'_i, t'_i) -\gamma) \right)
$$

where $\gamma$ is the margin, $\sigma$ is the sigmoid activation function and $(h'_i, r'_i, t'_i),\quad (i = 1, 2, \ldots, K)$ are ùêæ negative samples of (‚Ñé, ùëü, ùë°). The weight $p_i$ is the self-adversarial weights

- By minimizing the pre-training loss, the KG structural information such as subgraph structure and relational patterns is captured in the embeddings

### Step 2: Knowledge prefix adapter

- After learning the structural embedding, we need to project them into the same textual token representation space of $M$, using knowledge prefix adapter $P$ (usually a simple projection layer)

$$
K = P (ùíâ) ‚äï P (ùíì) ‚äï P (ùíï)¬†
$$

where $K$ is the knowledge token, (**ùíâ, ùíì, ùíï**) are the embeddings

- Hence, the input sequence:

$$
S_{ùëòùëùùëé} = K ‚äï I_{ùëñùë°} ‚äï X
$$

### Step 3: Fine tuning with $S_{kpa}$

- Finally, we froze the pre-trained structural embeddings and train the adapter to learn the mapping from structural knowledge toward textual representation
- LLM is further fine-tuned with $S_{ùëòùëùùëé}$

## Complexity Analysis

KoPA makes the length of the prompt more refined, as the length of virtual tokens generated by the structural prefix adapter is **fixed to 3** for head/relation/tail respectively

![Untitled](/images/Making%20Large%20Language%20Models%20Perform%20Better%20in%20Kno%205fdf3bcadeca4e1a950a604e114d3ee9/Untitled%201.png)

# Results

![Untitled](/images/Making%20Large%20Language%20Models%20Perform%20Better%20in%20Kno%205fdf3bcadeca4e1a950a604e114d3ee9/Untitled%202.png)

## Ablation study

Note that putting the virtual knowledge tokens generated by the adapter in the **middle (infix)** or in the **last (suffix)** of the input sequence will also decrease the performance.

- We believe because of the decoder-only architectures with **unidirectional self-attention**
- Putting the token in front of the sequence make the model pay more attention to the structural information

![Untitled](/images/Making%20Large%20Language%20Models%20Perform%20Better%20in%20Kno%205fdf3bcadeca4e1a950a604e114d3ee9/Untitled%203.png)

# Reference

1. Zhang et al. ‚Äú[Making Large Language Models Perform Better in Knowledge Graph Completion](https://arxiv.org/abs/2310.06671)‚Äù, arXiv preprint arXiv:2310.06671 (2023).